<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 9.5.12 (464966)"/><meta name="author" content="Ἅιδης"/><meta name="created" content="2021-03-09 14:03:03 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2021-03-09 14:23:26 +0000"/><title>6.4 GBDT算法</title></head><body><div>参考</div><div><a href="https://www.cnblogs.com/bnuvincent/p/9693190.html">https://www.cnblogs.com/bnuvincent/p/9693190.html</a></div><div><br/></div><div><a href="https://blog.csdn.net/yyy430/article/details/85108797">https://blog.csdn.net/yyy430/article/details/85108797</a></div><div><br/></div><div><br/></div><div><font style="font-size: 18px;">GBDT是通过采用加法模型（函数的线性组合），以及不断减小训练过程产生的残差达到将数据分类或者回归的方法。</font></div><div><img src="6.4%20GBDT%E7%AE%97%E6%B3%95.resources/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202021-03-10%2001.07.07.png" height="264" width="534"/></div><div><br/></div><div><font style="font-size: 18px;">GBDT通过多轮迭代，每迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。对弱分类器的要求是足够简单，并且是低方差和高偏差的。因为训练的过程是通过不断降低偏差来提高最终分类器的精度。弱分类器一般选择cart tree。由于上述高偏差的简单要求，每个分类回归树的深度不会很深。最终的分类器是将每轮训练得到的弱分类器加权求和得到的。</font></div><div><font style="font-size: 18px;"><br/></font></div><div><span style="font-size: 18px;"><img src="6.4%20GBDT%E7%AE%97%E6%B3%95.resources/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202021-03-10%2001.18.38.png" height="650" width="772"/></span></div><div><font style="font-size: 18px;"><br/></font></div><div style="padding: 0px; text-indent: 0px; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-variant-caps: normal; letter-spacing: normal; text-align: left; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; margin-top: 1em; margin-bottom: 1em;"><font style="font-size: 24px;"><span style="text-indent: 0px; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-variant-caps: normal; letter-spacing: normal; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; line-height: 1.5;-en-paragraph:true;">gbdt 通过经验风险极小化来确定下一个弱分类器的参数。<b>具体到损失函数本身的选择也就是L的选择，有平方损失函数，0-1损失函数，对数损失函数等等</b>。如果我们选择平方损失函数，那么这个差值其实就是我们平常所说的残差。</span></font></div><ul style="padding: 0px; word-break: break-all; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-variant-caps: normal; letter-spacing: normal; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><ul style=" padding: 0px; word-break: break-all;"><li style=" padding: 0px; list-style: circle;"><div><font style="font-size: 24px;">但是其实我们真正关注的，1.是希望损失函数能够不断的减小，2.是希望损失函数能够尽可能快的减小。所以如何尽可能快的减小呢？</font></div></li><li style=" padding: 0px; list-style: circle;"><div><font style="font-size: 24px;"><b>让损失函数沿着梯度方向的下降。这个就是gbdt的核心了。</b> 利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值去拟合一个回归树。gbdt 每轮迭代的时候，都去拟合损失函数在当前模型下的负梯度。</font></div></li></ul><ul style=" padding: 0px; word-break: break-all;"><li style=" padding: 0px; list-style: circle;"><div><font style="font-size: 24px;">这样每轮训练的时候都能够让损失函数尽可能快的减小，尽快的收敛达到局部最优解或者全局最优解。</font></div></li></ul></ul><div/><div><br/></div></body></html>